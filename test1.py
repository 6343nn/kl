import os
import time
import shutil # ƒê·ªÉ di chuy·ªÉn file
import cv2
import numpy as np
import torch
from PIL import Image
from diffusers import (
    StableDiffusionControlNetImg2ImgPipeline,
    ControlNetModel,
    UniPCMultistepScheduler,
)
from skimage.metrics import structural_similarity as ssim
from PIL import UnidentifiedImageError

#import brisque # B·ªè comment n·∫øu ƒë√£ c√†i ƒë·∫∑t v√† mu·ªën s·ª≠ d·ª•ng. C·∫ßn c√†i: pip install pybrisque

print("ƒê√£ import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt.")

# =============================================================
# ===                THI·∫æT L·∫¨P C·∫§U H√åNH                    ===
# =============================================================

# --- ƒê∆∞·ªùng d·∫´n ---
input_dir = "·∫£nh_ƒë√£_t√°ch_3"  # <<<--- !!! C·∫¨P NH·∫¨T ƒê∆Ø·ªúNG D·∫™N N√ÄY !!!
output_dir_base = "ket_qua_mammogram_final3"        # Th∆∞ m·ª•c g·ªëc cho output
output_accepted_dir = os.path.join(output_dir_base, "accepted") # Th∆∞ m·ª•c ch·ª©a ·∫£nh ch·∫•p nh·∫≠n
output_rejected_dir = os.path.join(output_dir_base, "rejected") # Th∆∞ m·ª•c ch·ª©a ·∫£nh b·ªã lo·∫°i

# --- Prompt v√† Negative Prompt ---
prompt = (
    "high-resolution mammogram, MLO view (mediolateral oblique), detailed anatomically accurate breast tissue structure, "
    "realistic grayscale contrast, high dynamic range, sharp focus, minimal artifacts, clean background, "
    "consistent BIRADS features, medically plausible appearance, photorealistic"
)
negative_prompt = (
    "low quality, worst quality, bad quality, blurry, blurred, noise, noisy, grain, grainy, text, signature, watermark, label, drawing, sketch, cartoon, illustration, painting, art, multiple views, multiple breasts, "
    "distorted anatomy, deformed, disfigured, unrealistic, unnatural, artifact, jpeg artifacts, compression artifacts, overexposed, underexposed, low contrast, bad contrast, "
    "implants, surgical clips, foreign object, metal, grid lines, markers, annotations, overlays, borders, frame, "
    "duplicate structures, fused anatomy, incorrect view (e.g., CC view instead of MLO), motion blur, patient movement, low resolution, pixelated"
)

# --- Tham s·ªë Sinh ·∫¢nh ---
strength_values = [0.15, 0.25, 0.35] # M·ª©c ƒë·ªô thay ƒë·ªïi so v·ªõi ·∫£nh g·ªëc/canny (th·∫•p -> gi·ªØ c·∫•u tr√∫c)
guidance_values = [3.0, 4.5, 6.0]    # M·ª©c ƒë·ªô b√°m s√°t prompt (th·∫•p-trung b√¨nh -> t·ª± nhi√™n h∆°n)
num_inference_steps = 30             # S·ªë b∆∞·ªõc kh·ª≠ nhi·ªÖu (tƒÉng -> chi ti·∫øt h∆°n, ch·∫≠m h∆°n)
# controlnet_scales = [0.9, 1.0, 1.1]  # T√πy ch·ªçn: Tr·ªçng s·ªë ·∫£nh h∆∞·ªüng c·ªßa ControlNet

# --- Tham s·ªë kh√°c ---
pixel_to_cm = 0.05                   # T·ª∑ l·ªá chuy·ªÉn ƒë·ªïi pixel sang cm (cho ƒë√°nh gi√° contour)
pipeline_size = (512, 512)           # K√≠ch th∆∞·ªõc ·∫£nh input cho pipeline (th∆∞·ªùng l√† 512 ho·∫∑c 768)
use_adaptive_canny = True            # True: D√πng ng∆∞·ª°ng canny t·ª± ƒë·ªông; False: D√πng ng∆∞·ª°ng c·ªë ƒë·ªãnh b√™n d∆∞·ªõi
canny_lower_threshold = 100          # Ng∆∞·ª°ng d∆∞·ªõi c·ªë ƒë·ªãnh (ch·ªâ d√πng n·∫øu use_adaptive_canny=False)
canny_upper_threshold = 200          # Ng∆∞·ª°ng tr√™n c·ªë ƒë·ªãnh (ch·ªâ d√πng n·∫øu use_adaptive_canny=False)
reproducible_seed = False            # True: D√πng seed c·ªë ƒë·ªãnh (t√°i l·∫≠p); False: D√πng seed ng·∫´u nhi√™n (ƒëa d·∫°ng)
base_seed = 42                       # Seed g·ªëc (ch·ªâ d√πng n·∫øu reproducible_seed=True)

# --- Ng∆∞·ª°ng L·ªçc T·ª± ƒë·ªông ---
# <<<--- !!! QUAN TR·ªåNG: C·∫¶N ƒêI·ªÄU CH·ªàNH C√ÅC NG∆Ø·ª†NG N√ÄY SAU KHI TH·ª¨ NGHI·ªÜM !!! ---<<<
ENABLE_CONTOUR_FILTER = True         # True ƒë·ªÉ b·∫≠t l·ªçc theo contour
MAX_ACCEPTABLE_AVG_DIFF_CM = 1.0     # Ng∆∞·ª°ng max cho ƒë·ªô l·ªách contour TRUNG B√åNH (cm)
MAX_ACCEPTABLE_MAX_DIFF_CM = 2.5     # Ng∆∞·ª°ng max cho ƒë·ªô l·ªách contour L·ªöN NH·∫§T (cm)

ENABLE_SSIM_FILTER = True            # True ƒë·ªÉ b·∫≠t l·ªçc theo SSIM
MIN_ACCEPTABLE_SSIM = 0.65           # Ng∆∞·ª°ng SSIM T·ªêI THI·ªÇU so v·ªõi ·∫£nh g·ªëc (0-1, c√†ng cao c√†ng gi·ªëng)

ENABLE_BRISQUE_FILTER = False        # True ƒë·ªÉ b·∫≠t l·ªçc theo BRISQUE (c·∫ßn c√†i pybrisque)
# MAX_ACCEPTABLE_BRISQUE = 40        # Ng∆∞·ª°ng BRISQUE T·ªêI ƒêA (ƒëi·ªÉm c√†ng th·∫•p, ch·∫•t l∆∞·ª£ng c√†ng t·ªët)
# <<<--------------------------------------------------------------------------->>>

# --- Thi·∫øt l·∫≠p Device v√† ƒê·ªô ch√≠nh x√°c ---
device = "cuda" if torch.cuda.is_available() else "cpu"
use_float16 = device == "cuda" # S·ª≠ d·ª•ng float16 tr√™n GPU cho hi·ªáu nƒÉng t·ªët h∆°n

print(f"\nüöÄ S·ª≠ d·ª•ng thi·∫øt b·ªã: {device.upper()}")
if use_float16: print("‚ö° S·ª≠ d·ª•ng ƒë·ªô ch√≠nh x√°c Float16 tr√™n GPU.")
else: print("üßä S·ª≠ d·ª•ng ƒë·ªô ch√≠nh x√°c Float32 tr√™n CPU (s·∫Ω ch·∫≠m h∆°n).")

# T·∫°o c√°c th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
os.makedirs(output_accepted_dir, exist_ok=True)
os.makedirs(output_rejected_dir, exist_ok=True)
print(f"üìÇ Th∆∞ m·ª•c ·∫£nh ch·∫•p nh·∫≠n: {os.path.abspath(output_accepted_dir)}")
print(f"üóëÔ∏è Th∆∞ m·ª•c ·∫£nh b·ªã lo·∫°i: {os.path.abspath(output_rejected_dir)}")

# =============================================================
# ===                   H√ÄM PH·ª§ TR·ª¢                       ===
# =============================================================

def get_lower_contour(img_array_gray, crop_ratio=0.3):
    """T√¨m contour d∆∞·ªõi c√πng c·ªßa ·∫£nh t·ª´ m·∫£ng NumPy thang ƒë·ªô x√°m."""
    if img_array_gray is None or img_array_gray.ndim != 2:
        print("    ‚ö†Ô∏è [Contour] ƒê·∫ßu v√†o kh√¥ng h·ª£p l·ªá.")
        return None, (0, 0)
    h, w = img_array_gray.shape
    if h == 0 or w == 0: return None, (0, 0)
    start_row = int(h * (1 - crop_ratio))
    lower_crop = img_array_gray[start_row:, ]
    if lower_crop.size == 0: return None, (h, w) # Crop kh√¥ng th√†nh c√¥ng

    ksize = (5, 5)
    if lower_crop.shape[0] < ksize[0] or lower_crop.shape[1] < ksize[1]:
        ksize = (min(3, max(1, lower_crop.shape[0] // 2 * 2 + 1)), min(3, max(1, lower_crop.shape[1] // 2 * 2 + 1)))

    try:
        # √Åp d·ª•ng Gaussian Blur v√† Threshold
        blurred = cv2.GaussianBlur(lower_crop, ksize, 0)
        _, thresh = cv2.threshold(blurred, 30, 255, cv2.THRESH_BINARY)
        # T√¨m contours
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if contours:
            # T√¨m contour l·ªõn nh·∫•t theo di·ªán t√≠ch
            max_contour = max(contours, key=cv2.contourArea)
            # ƒêi·ªÅu ch·ªânh t·ªça ƒë·ªô y c·ªßa contour v·ªÅ h·ªá t·ªça ƒë·ªô ·∫£nh g·ªëc
            max_contour[:, :, 1] += start_row
            return max_contour, img_array_gray.shape
        else:
            # print("    ‚ö†Ô∏è [Contour] Kh√¥ng t√¨m th·∫•y contour n√†o.")
            return None, img_array_gray.shape
    except cv2.error as e:
         print(f"    ‚ö†Ô∏è [Contour] L·ªói OpenCV: {e}")
         return None, img_array_gray.shape

def compare_contours(contour1, contour2, shape):
    """So s√°nh hai contours d·ª±a tr√™n s·ª± kh√°c bi·ªát trung b√¨nh v√† l·ªõn nh·∫•t theo tr·ª•c y."""
    h, w = shape
    if contour1 is None or contour2 is None or not contour1.size or not contour2.size: return None, None
    diffs = []
    y_coords1 = {}
    y_coords2 = {}
    # T·∫°o c·∫•u tr√∫c d·ªØ li·ªáu ƒë·ªÉ truy c·∫≠p nhanh c√°c ƒëi·ªÉm y theo x
    for pt in contour1: x, y = pt[0]; y_coords1.setdefault(x, []).append(y)
    for pt in contour2: x, y = pt[0]; y_coords2.setdefault(x, []).append(y)
    # Duy·ªát qua c√°c c·ªôt x c√≥ ƒëi·ªÉm ·ªü c·∫£ hai contour
    common_x = set(y_coords1.keys()) & set(y_coords2.keys())
    if not common_x:
        # print("    ‚ö†Ô∏è [Contour Compare] Kh√¥ng c√≥ ƒëi·ªÉm x chung.")
        return None, None
    for x in common_x:
        mean_y1 = np.mean(y_coords1[x])
        mean_y2 = np.mean(y_coords2[x])
        diffs.append(abs(mean_y1 - mean_y2))
    if diffs: return np.mean(diffs), np.max(diffs)
    else: return None, None # Kh√¥ng n√™n x·∫£y ra n·∫øu common_x kh√¥ng r·ªóng

def get_adaptive_canny(image_np_gray, sigma=0.33):
    """T·∫°o ·∫£nh Canny v·ªõi ng∆∞·ª°ng t·ª± ƒë·ªông d·ª±a tr√™n median."""
    # L√†m m·ªù nh·∫π ƒë·ªÉ gi·∫£m nhi·ªÖu tr∆∞·ªõc khi t√≠nh median
    blurred = cv2.GaussianBlur(image_np_gray, (3, 3), 0)
    v = np.median(blurred)
    # √Åp d·ª•ng c√¥ng th·ª©c Canny t·ª± ƒë·ªông
    lower = int(max(0, (1.0 - sigma) * v))
    upper = int(min(255, (1.0 + sigma) * v))
    # print(f"    - Ng∆∞·ª°ng Canny t·ª± ƒë·ªông: ({lower}, {upper})") # B·ªè comment n·∫øu mu·ªën xem ng∆∞·ª°ng
    edged = cv2.Canny(image_np_gray, lower, upper)
    return edged

# =============================================================
# ===                    T·∫¢I MODEL AI                      ===
# =============================================================
controlnet = None
pipe = None
model_load_successful = False
try:
    start_load_time = time.time()
    print("\n‚è≥ ƒêang t·∫£i ControlNet model (lllyasviel/sd-controlnet-canny)...")
    controlnet = ControlNetModel.from_pretrained(
        "lllyasviel/sd-controlnet-canny",
        torch_dtype=torch.float16 if use_float16 else torch.float32
    )
    print("‚úÖ ControlNet model ƒë√£ t·∫£i xong.")

    print("‚è≥ ƒêang t·∫£i Stable Diffusion pipeline (runwayml/stable-diffusion-v1-5)...")
    pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        controlnet=controlnet,
        torch_dtype=torch.float16 if use_float16 else torch.float32,
        safety_checker=None # T·∫Øt safety checker (ph·ªï bi·∫øn khi d√πng ControlNet t√πy ch·ªânh)
    ).to(device)
    print(f"‚úÖ Stable Diffusion pipeline ƒë√£ t·∫£i xong v√† chuy·ªÉn sang device '{device}'.")

    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
    print("‚úÖ ƒê√£ c·∫•u h√¨nh Scheduler (UniPCMultistepScheduler).")

    if device == "cuda":
        try:
            pipe.enable_xformers_memory_efficient_attention()
            print("‚úÖ ƒê√£ b·∫≠t Xformers memory efficient attention (tƒÉng t·ªëc/gi·∫£m b·ªô nh·ªõ).")
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ b·∫≠t Xformers: {e}. C√≥ th·ªÉ ·∫£nh h∆∞·ªüng hi·ªáu nƒÉng/b·ªô nh·ªõ.")
        # C√¢n nh·∫Øc b·∫≠t n·∫øu VRAM c·ª±c k·ª≥ h·∫°n ch·∫ø (< 6GB), nh∆∞ng s·∫Ω ch·∫≠m h∆°n ƒë√°ng k·ªÉ:
        # try:
        #      pipe.enable_model_cpu_offload()
        #      print("‚úÖ ƒê√£ b·∫≠t CPU offloading (cho VRAM th·∫•p).")
        # except Exception as e:
        #      print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ b·∫≠t CPU offloading: {e}.")

    end_load_time = time.time()
    print(f"‚è±Ô∏è T·ªïng th·ªùi gian t·∫£i model: {end_load_time - start_load_time:.2f} gi√¢y")
    model_load_successful = True

except Exception as e:
    print(f"\n‚ùå L·ªói nghi√™m tr·ªçng khi t·∫£i ho·∫∑c c·∫•u h√¨nh model AI: {e}")
    print("   Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi m·∫°ng, dung l∆∞·ª£ng ƒëƒ©a v√† c√†i ƒë·∫∑t th∆∞ vi·ªán (ƒë·∫∑c bi·ªát l√† PyTorch v·ªõi CUDA n·∫øu d√πng GPU).")
    print("   Tho√°t ch∆∞∆°ng tr√¨nh.")
    exit()

if not model_load_successful:
     print("\n‚ùå Kh√¥ng th·ªÉ t·∫£i model AI. Tho√°t ch∆∞∆°ng tr√¨nh.")
     exit()

# =============================================================
# ===                 CH·ªåN ·∫¢NH ƒê·∫¶U V√ÄO                    ===
# =============================================================
if not os.path.isdir(input_dir):
    print(f"‚ùå Th∆∞ m·ª•c ƒë·∫ßu v√†o '{input_dir}' kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng ph·∫£i l√† th∆∞ m·ª•c.")
    exit()

try:
    all_images = sorted([f for f in os.listdir(input_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.webp'))])
    num_available_images = len(all_images)
except Exception as e:
    print(f"‚ùå L·ªói khi ƒë·ªçc th∆∞ m·ª•c ƒë·∫ßu v√†o '{input_dir}': {e}")
    exit()


if not all_images:
    print(f"‚ùå Kh√¥ng t√¨m th·∫•y file ·∫£nh n√†o ƒë∆∞·ª£c h·ªó tr·ª£ trong th∆∞ m·ª•c '{input_dir}'.")
    exit()

print(f"\nüñºÔ∏è T√¨m th·∫•y t·ªïng c·ªông {num_available_images} ·∫£nh trong th∆∞ m·ª•c ƒë·∫ßu v√†o.")
# Ch·ªâ hi·ªÉn th·ªã m·ªôt ph·∫ßn danh s√°ch n·∫øu qu√° d√†i
max_display = 30
if num_available_images > 0:
    print("   Danh s√°ch (m·ªôt ph·∫ßn):")
    for idx, img_name in enumerate(all_images):
        if idx < max_display:
            print(f"     {idx+1}. {img_name}")
        elif idx == max_display:
            print(f"     ... v√† {num_available_images - max_display} ·∫£nh kh√°c.")
            break
else:
     print("   Th∆∞ m·ª•c kh√¥ng ch·ª©a ·∫£nh n√†o.")
     exit()


selected_images_paths = []
while not selected_images_paths:
    print("\n" + "-"*30)
    print("L·ª±a ch·ªçn ·∫£nh ƒë·ªÉ x·ª≠ l√Ω:")
    print("  - Nh·∫≠p m·ªôt s·ªë N: Ch·ªçn c√°c ·∫£nh t·ª´ 1 ƒë·∫øn N.")
    print("  - Nh·∫≠p danh s√°ch s·ªë c√°ch nhau b·ªüi d·∫•u ph·∫©y (VD: 1,5,10): Ch·ªçn c√°c ·∫£nh c·ª• th·ªÉ.")
    selected_input = input(f"üëâ Nh·∫≠p l·ª±a ch·ªçn c·ªßa b·∫°n: ")
    selected_input = selected_input.strip()

    valid_indices = [] # L∆∞u index 0-based
    invalid_inputs = []

    if not selected_input:
        print("   ‚ùå Vui l√≤ng nh·∫≠p l·ª±a ch·ªçn.")
        continue

    if ',' in selected_input:
        # --- X·ª≠ l√Ω danh s√°ch s·ªë ---
        parts = selected_input.split(',')
        potential_indices = set()
        for part in parts:
            part = part.strip()
            if not part: continue
            try:
                index_num = int(part) # S·ªë ng∆∞·ªùi d√πng nh·∫≠p (1-based)
                if 1 <= index_num <= num_available_images:
                    potential_indices.add(index_num - 1) # Chuy·ªÉn sang 0-based
                else:
                    invalid_inputs.append(part)
            except ValueError:
                invalid_inputs.append(part)
        valid_indices = sorted(list(potential_indices))
    else:
        # --- X·ª≠ l√Ω m·ªôt s·ªë N ---
        try:
            num_to_select = int(selected_input)
            if 1 <= num_to_select <= num_available_images:
                valid_indices = list(range(num_to_select)) # Index t·ª´ 0 ƒë·∫øn N-1
            elif num_to_select <= 0:
                 print("   ‚ùå S·ªë l∆∞·ª£ng ·∫£nh ch·ªçn ph·∫£i l√† s·ªë d∆∞∆°ng.")
                 invalid_inputs.append(selected_input)
            else: # num_to_select > num_available_images
                print(f"   ‚ùå Ch·ªâ c√≥ {num_available_images} ·∫£nh, kh√¥ng th·ªÉ ch·ªçn t·ª´ 1 ƒë·∫øn {num_to_select}.")
                invalid_inputs.append(selected_input)
        except ValueError:
            print("   ‚ùå Nh·∫≠p li·ªáu kh√¥ng h·ª£p l·ªá. Ch·ªâ nh·∫≠p s·ªë ho·∫∑c danh s√°ch s·ªë.")
            invalid_inputs.append(selected_input)

    if invalid_inputs:
        print(f"   ‚ö†Ô∏è C√°c m·ª•c nh·∫≠p sau kh√¥ng h·ª£p l·ªá ho·∫∑c ngo√†i ph·∫°m vi [1-{num_available_images}] ƒë√£ b·ªã b·ªè qua: {invalid_inputs}")

    if not valid_indices:
        print("   ‚ùå Kh√¥ng c√≥ ·∫£nh h·ª£p l·ªá n√†o ƒë∆∞·ª£c ch·ªçn. Vui l√≤ng th·ª≠ l·∫°i.")
    else:
        selected_images_paths = [os.path.join(input_dir, all_images[i]) for i in valid_indices]
        print(f"‚úÖ ƒê√£ ch·ªçn {len(selected_images_paths)} ·∫£nh ƒë·ªÉ x·ª≠ l√Ω.")
        if len(valid_indices) > 1:
             print(f"   (S·ªë th·ª© t·ª± t·ª´ {valid_indices[0]+1} ƒë·∫øn {valid_indices[-1]+1} n·∫øu ch·ªçn theo kho·∫£ng, ho·∫∑c c√°c s·ªë c·ª• th·ªÉ)")
        elif len(valid_indices) == 1:
             print(f"   (S·ªë th·ª© t·ª±: {valid_indices[0]+1})")

# =============================================================
# ===               X·ª¨ L√ù CH√çNH T·ª™NG ·∫¢NH                  ===
# =============================================================
total_start_time = time.time()
total_generated_count = 0
total_accepted_count = 0
total_rejected_count = 0
total_error_count = 0
total_contour_fail_count = 0

print(f"\n{'='*20} B·∫ÆT ƒê·∫¶U X·ª¨ L√ù {len(selected_images_paths)} ·∫¢NH {'='*20}")

# --- T√≠nh to√°n t·ªïng s·ªë l·∫ßn t·∫°o ·∫£nh d·ª± ki·∫øn ---
num_combinations = len(strength_values) * len(guidance_values)
# if controlnet_scales: num_combinations *= len(controlnet_scales) # B·ªè comment n·∫øu d√πng controlnet_scales
estimated_total_generations = len(selected_images_paths) * num_combinations
print(f"‚öôÔ∏è C·∫•u h√¨nh: {num_combinations} phi√™n b·∫£n cho m·ªói ·∫£nh g·ªëc.")
print(f"‚è≥ ∆Ø·ªõc t√≠nh t·ªïng s·ªë l·∫ßn t·∫°o ·∫£nh: {estimated_total_generations}")
print("-" * 60)

# --- B·∫Øt ƒë·∫ßu v√≤ng l·∫∑p x·ª≠ l√Ω ---
for img_idx, img_path in enumerate(selected_images_paths):
    img_process_start_time = time.time()
    base_name = os.path.splitext(os.path.basename(img_path))[0]

    # T·∫°o th∆∞ m·ª•c con cho t·ª´ng ·∫£nh g·ªëc trong accepted v√† rejected
    sub_output_accepted_dir = os.path.join(output_accepted_dir, base_name)
    sub_output_rejected_dir = os.path.join(output_rejected_dir, base_name)
    os.makedirs(sub_output_accepted_dir, exist_ok=True)
    os.makedirs(sub_output_rejected_dir, exist_ok=True)

    print(f"\n--- [{img_idx+1}/{len(selected_images_paths)}] üîç B·∫Øt ƒë·∫ßu x·ª≠ l√Ω ·∫£nh: {base_name} ---")
    print(f"    ƒê∆∞·ªùng d·∫´n g·ªëc: {img_path}")

    accepted_count_per_image = 0
    generation_count_per_image = 0
    error_count_per_image = 0
    contour_orig = None
    original_image_pil = None
    original_image_np_gray = None

    try:
        # --- 1. ƒê·ªçc v√† Chu·∫©n b·ªã ·∫¢nh G·ªëc ---
        print("    1. ƒê·ªçc v√† chu·∫©n b·ªã ·∫£nh g·ªëc...")
        original_image_pil = Image.open(img_path).convert("RGB")
        original_size_actual = original_image_pil.size # L·∫•y k√≠ch th∆∞·ªõc th·ª±c t·∫ø (width, height)

        # L∆∞u ·∫£nh g·ªëc v√†o th∆∞ m·ª•c accepted ƒë·ªÉ tham chi·∫øu
        original_save_path = os.path.join(sub_output_accepted_dir, f"{base_name}_original.png")
        original_image_pil.save(original_save_path)
        print(f"       üíæ ƒê√£ l∆∞u ·∫£nh g·ªëc v√†o: {os.path.relpath(original_save_path)}")

        # Chuy·ªÉn sang NumPy (thang ƒë·ªô x√°m)
        original_image_np_gray = np.array(original_image_pil.convert("L"))

        # --- 2. L·∫•y Contour G·ªëc ---
        print("    2. T√¨m contour ·∫£nh g·ªëc...")
        contour_orig, shape_orig = get_lower_contour(original_image_np_gray)
        if contour_orig is None:
            print("       ‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c contour g·ªëc h·ª£p l·ªá. B·ªè qua ·∫£nh n√†y.")
            total_contour_fail_count += 1
            # Di chuy·ªÉn ·∫£nh g·ªëc v√†o th∆∞ m·ª•c rejected ƒë·ªÉ d·ªÖ ki·ªÉm tra
            try:
                 shutil.move(original_save_path, os.path.join(sub_output_rejected_dir, os.path.basename(original_save_path)))
            except Exception as move_err:
                 print(f"       L·ªói khi di chuy·ªÉn file g·ªëc b·ªã l·ªói contour: {move_err}")
            continue # Chuy·ªÉn sang ·∫£nh ti·∫øp theo trong v√≤ng l·∫∑p for img_path

        print("       ‚úÖ T√¨m th·∫•y contour g·ªëc.")

        # --- 3. Chu·∫©n b·ªã ·∫¢nh cho Pipeline AI ---
        print("    3. Chu·∫©n b·ªã ·∫£nh cho pipeline...")
        pipeline_input_image_pil = original_image_pil.resize(pipeline_size, Image.LANCZOS)
        pipeline_input_image_np = np.array(pipeline_input_image_pil) # ·∫¢nh RGB cho Canny
        pipeline_input_gray_np = cv2.cvtColor(pipeline_input_image_np, cv2.COLOR_RGB2GRAY) # ·∫¢nh Gray cho Canny

        # T·∫°o ·∫£nh Canny (Th√≠ch ·ª©ng ho·∫∑c C·ªë ƒë·ªãnh)
        if use_adaptive_canny:
            canny_np = get_adaptive_canny(pipeline_input_gray_np)
            # print("       - ƒê√£ t·∫°o Canny map th√≠ch ·ª©ng.")
        else:
            canny_np = cv2.Canny(pipeline_input_gray_np, canny_lower_threshold, canny_upper_threshold)
            # print(f"       - ƒê√£ t·∫°o Canny map c·ªë ƒë·ªãnh ({canny_lower_threshold}, {canny_upper_threshold}).")

        canny_image_pil = Image.fromarray(cv2.cvtColor(canny_np, cv2.COLOR_GRAY2RGB))

        # L∆∞u ·∫£nh Canny v√†o th∆∞ m·ª•c accepted ƒë·ªÉ tham chi·∫øu
        canny_save_path = os.path.join(sub_output_accepted_dir, f"{base_name}_canny_control.png")
        canny_image_pil.save(canny_save_path)
        # print(f"       üíæ ƒê√£ l∆∞u ·∫£nh Canny control v√†o: {os.path.relpath(canny_save_path)}")
        print("       ‚úÖ ·∫¢nh ƒë·∫ßu v√†o v√† Canny map ƒë√£ s·∫µn s√†ng.")


        # --- 4. V√≤ng l·∫∑p T·∫°o ·∫¢nh v√† L·ªçc ---
        print(f"    4. B·∫Øt ƒë·∫ßu t·∫°o {num_combinations} phi√™n b·∫£n ·∫£nh...")

        for strength in strength_values:
            for guidance in guidance_values:
                # for control_scale in controlnet_scales: # B·ªè comment n·∫øu d√πng controlnet_scale
                    gen_start_time = time.time()
                    generation_count_per_image += 1
                    total_generated_count += 1

                    # --- T·∫°o t√™n file output ---
                    # C·∫≠p nh·∫≠t t√™n file n·∫øu d√πng controlnet_scale:
                    # out_name = f"{base_name}_s{strength}_g{guidance}_cs{control_scale}.png"
                    out_name = f"{base_name}_s{strength}_g{guidance}.png"
                    progress_percent = (total_generated_count / estimated_total_generations) * 100 if estimated_total_generations > 0 else 0

                    print(f"       ‚è≥ [{generation_count_per_image}/{num_combinations} | T·ªïng: {total_generated_count}/{estimated_total_generations} ({progress_percent:.1f}%)] T·∫°o: {out_name} ...")

                    # --- Th·ª±c hi·ªán Inference ---
                    try:
                        # T·∫°o seed (T√°i l·∫≠p ho·∫∑c Ng·∫´u nhi√™n)
                        if reproducible_seed:
                             current_seed = base_seed + img_idx * num_combinations + generation_count_per_image
                        else:
                             current_seed = int(time.time() * 1000) + generation_count_per_image # Seed ng·∫´u nhi√™n thay ƒë·ªïi
                        generator = torch.Generator(device=device).manual_seed(current_seed)

                        # G·ªçi pipeline AI
                        result_pil = pipe(
                            prompt=prompt,
                            negative_prompt=negative_prompt,
                            image=pipeline_input_image_pil,      # ·∫¢nh m√†u ƒë√£ resize
                            control_image=canny_image_pil,   # ·∫¢nh Canny 3 k√™nh
                            strength=strength,
                            guidance_scale=guidance,
                            num_inference_steps=num_inference_steps,
                            # controlnet_conditioning_scale=control_scale, # B·ªè comment n·∫øu d√πng
                            generator=generator
                        ).images[0]

                        # --- H·∫≠u x·ª≠ l√Ω k·∫øt qu·∫£ ---
                        result_resized_pil = result_pil.resize(original_size_actual, Image.LANCZOS)
                        result_resized_np_gray = np.array(result_resized_pil.convert("L")) # Chuy·ªÉn sang Gray ƒë·ªÉ ƒë√°nh gi√°

                        # --- ƒê√°nh gi√° v√† L·ªçc ---
                        contour_gen, _ = get_lower_contour(result_resized_np_gray)

                        contour_ok = not ENABLE_CONTOUR_FILTER # M·∫∑c ƒë·ªãnh OK n·∫øu kh√¥ng b·∫≠t filter
                        ssim_ok = not ENABLE_SSIM_FILTER       # M·∫∑c ƒë·ªãnh OK n·∫øu kh√¥ng b·∫≠t filter
                        brisque_ok = not ENABLE_BRISQUE_FILTER # M·∫∑c ƒë·ªãnh OK n·∫øu kh√¥ng b·∫≠t filter

                        log_details = [] # Thu th·∫≠p th√¥ng tin chi ti·∫øt ƒë·ªÉ in
                        avg_diff_cm, max_diff_cm, similarity_index, brisque_score = None, None, None, None

                        # 1. ƒê√°nh gi√° Contour (n·∫øu b·∫≠t)
                        if ENABLE_CONTOUR_FILTER:
                            if contour_gen is None:
                                log_details.append("ContourGen:‚õî")
                                contour_ok = False
                            else:
                                avg_diff, max_diff = compare_contours(contour_orig, contour_gen, shape_orig)
                                if avg_diff is not None:
                                    avg_diff_cm = avg_diff * pixel_to_cm
                                    max_diff_cm = max_diff * pixel_to_cm
                                    log_details.append(f"ŒîTB:{avg_diff_cm:.2f}cm")
                                    log_details.append(f"ŒîMAX:{max_diff_cm:.2f}cm")
                                    if avg_diff_cm <= MAX_ACCEPTABLE_AVG_DIFF_CM and max_diff_cm <= MAX_ACCEPTABLE_MAX_DIFF_CM:
                                        contour_ok = True
                                        log_details[-1] += "‚úÖ" # Th√™m tick v√†o log cu·ªëi
                                        log_details[-2] += "‚úÖ"
                                    else:
                                        contour_ok = False
                                        log_details[-1] += "‚õî" # Th√™m X v√†o log cu·ªëi
                                        log_details[-2] += "‚õî"
                                else:
                                    log_details.append("ContourCmp:‚ö†Ô∏è") # Kh√¥ng so s√°nh ƒë∆∞·ª£c
                                    contour_ok = False
                        else:
                            log_details.append("ContourFilt:OFF")

                        # 2. ƒê√°nh gi√° SSIM (n·∫øu b·∫≠t)
                        if ENABLE_SSIM_FILTER:
                            try:
                                if original_image_np_gray.shape == result_resized_np_gray.shape:
                                    similarity_index = ssim(original_image_np_gray, result_resized_np_gray, data_range=255)
                                    log_details.append(f"SSIM:{similarity_index:.3f}")
                                    if similarity_index >= MIN_ACCEPTABLE_SSIM:
                                        ssim_ok = True
                                        log_details[-1] += "‚úÖ"
                                    else:
                                        ssim_ok = False
                                        log_details[-1] += "‚õî"
                                else:
                                    log_details.append("SSIM:‚ö†Ô∏è(Size)")
                                    ssim_ok = False
                            except Exception as ssim_e:
                                log_details.append(f"SSIM:‚ö†Ô∏è(Err)")
                                print(f"        L·ªói SSIM: {ssim_e}")
                                ssim_ok = False
                        else:
                             log_details.append("SSIMFilt:OFF")

                        # 3. ƒê√°nh gi√° BRISQUE (n·∫øu b·∫≠t v√† ƒë√£ c√†i ƒë·∫∑t)
                        if ENABLE_BRISQUE_FILTER:
                            try:
                                import brisque # Import ·ªü ƒë√¢y ƒë·ªÉ tr√°nh l·ªói n·∫øu ch∆∞a c√†i
                                brisque_score = brisque.score(result_resized_np_gray)
                                log_details.append(f"BRISQUE:{brisque_score:.2f}")
                                if brisque_score <= MAX_ACCEPTABLE_BRISQUE:
                                    brisque_ok = True
                                    log_details[-1] += "‚úÖ"
                                else:
                                    brisque_ok = False
                                    log_details[-1] += "‚õî"
                            except ImportError:
                                log_details.append("BRISQUE:‚ö†Ô∏è(NotInstalled)")
                                brisque_ok = True # Coi nh∆∞ OK n·∫øu ch∆∞a c√†i
                            except Exception as brisque_e:
                                log_details.append(f"BRISQUE:‚ö†Ô∏è(Err)")
                                print(f"        L·ªói BRISQUE: {brisque_e}")
                                brisque_ok = False # Coi l√† l·ªói n·∫øu c√≥ l·ªói t√≠nh to√°n
                        # else: # Kh√¥ng c·∫ßn th√™m log n·∫øu filter OFF


                        # --- Quy·∫øt ƒë·ªãnh cu·ªëi c√πng ---
                        is_accepted = contour_ok and ssim_ok and brisque_ok

                        gen_end_time = time.time()
                        log_status = "‚úÖ ACCEPTED" if is_accepted else "‚õî REJECTED"
                        log_string = f"       -> {log_status} | {' | '.join(log_details)} | (Time: {gen_end_time - gen_start_time:.2f}s)"
                        print(log_string)

                        # --- L∆∞u v√†o th∆∞ m·ª•c t∆∞∆°ng ·ª©ng ---
                        if is_accepted:
                            total_accepted_count += 1
                            accepted_count_per_image += 1
                            out_path = os.path.join(sub_output_accepted_dir, out_name)
                            result_resized_pil.save(out_path)
                        else:
                            total_rejected_count += 1
                            out_path = os.path.join(sub_output_rejected_dir, out_name)
                            result_resized_pil.save(out_path)

                    # --- X·ª≠ l√Ω l·ªói trong qu√° tr√¨nh Inference ---
                    except torch.cuda.OutOfMemoryError:
                         print(f"       ‚ùå L·ªói OutOfMemoryError khi t·∫°o {out_name}! C√≥ th·ªÉ c·∫ßn gi·∫£m pipeline_size ho·∫∑c d√πng CPU offload.")
                         total_error_count += 1
                         error_count_per_image += 1
                         if device == 'cuda': torch.cuda.empty_cache() # C·ªë g·∫Øng gi·∫£i ph√≥ng b·ªô nh·ªõ
                         time.sleep(1)
                         continue # B·ªè qua l·∫ßn l·∫∑p n√†y
                    except Exception as infer_e:
                        gen_end_time = time.time()
                        print(f"       ‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh khi t·∫°o {out_name}: {infer_e} (Time: {gen_end_time - gen_start_time:.2f}s)")
                        total_error_count += 1
                        error_count_per_image += 1
                        continue # B·ªè qua l·∫ßn l·∫∑p n√†y

        print(f"    ‚úÖ Ho√†n th√†nh t·∫°o c√°c phi√™n b·∫£n cho {base_name}.")

    # --- X·ª≠ l√Ω l·ªói khi ƒë·ªçc/chu·∫©n b·ªã ·∫£nh g·ªëc ---
    except FileNotFoundError:
        print(f"‚ùå L·ªói FileNotFoundError: Kh√¥ng t√¨m th·∫•y file ·∫£nh: {img_path}")
        total_error_count += 1
    except UnidentifiedImageError:
         print(f"‚ùå L·ªói UnidentifiedImageError: Kh√¥ng th·ªÉ ƒë·ªçc ho·∫∑c file b·ªã h·ªèng: {img_path}")
         total_error_count += 1
    except Exception as prep_e:
        print(f"‚ùå L·ªói kh√¥ng mong mu·ªën khi x·ª≠ l√Ω file {base_name}: {prep_e}")
        import traceback
        traceback.print_exc() # In chi ti·∫øt l·ªói ƒë·ªÉ debug
        total_error_count += 1

    # --- T·ªïng k·∫øt cho ·∫£nh hi·ªán t·∫°i ---
    img_process_end_time = time.time()
    print(f"    ‚è±Ô∏è Ho√†n th√†nh x·ª≠ l√Ω {base_name} sau {img_process_end_time - img_process_start_time:.2f} gi√¢y.")
    print(f"       K·∫øt qu·∫£: {accepted_count_per_image} ch·∫•p nh·∫≠n, {generation_count_per_image - accepted_count_per_image - error_count_per_image} b·ªã lo·∫°i, {error_count_per_image} l·ªói.")
    print("-" * 60)

    # --- Gi·∫£i ph√≥ng b·ªô nh·ªõ GPU ---
    # X√≥a c√°c bi·∫øn kh√¥ng c·∫ßn thi·∫øt n·ªØa
    del original_image_pil, original_image_np_gray, contour_orig
    del pipeline_input_image_pil, pipeline_input_image_np, pipeline_input_gray_np, canny_image_pil
    # G·ªçi garbage collector v√† l√†m tr·ªëng cache CUDA
    import gc
    gc.collect()
    if device == 'cuda':
        torch.cuda.empty_cache()

# =============================================================
# ===                   T·ªîNG K·∫æT CU·ªêI C√ôNG                 ===
# =============================================================
total_end_time = time.time()
total_duration = total_end_time - total_start_time

print(f"\n{'='*25} HO√ÄN TH√ÄNH T·∫§T C·∫¢ {'='*25}")
print(f"T·ªïng th·ªùi gian th·ª±c thi: {total_duration:.2f} gi√¢y ({total_duration/60:.2f} ph√∫t)")
print("-" * 65)
print("Th·ªëng k√™:")
print(f"  - T·ªïng s·ªë ·∫£nh g·ªëc ƒë√£ x·ª≠ l√Ω: {len(selected_images_paths)}")
print(f"  - T·ªïng s·ªë ·∫£nh ƒë√£ t·∫°o:      {total_generated_count}")
print(f"  - S·ªë ·∫£nh ƒë∆∞·ª£c ch·∫•p nh·∫≠n:   {total_accepted_count}")
print(f"  - S·ªë ·∫£nh b·ªã lo·∫°i b·ªè:      {total_rejected_count}")
print(f"  - S·ªë l·∫ßn t·∫°o ·∫£nh g·∫∑p l·ªói: {total_error_count}")
print(f"  - S·ªë ·∫£nh g·ªëc b·ªã l·ªói contour: {total_contour_fail_count}")
print("-" * 65)
print(f"K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o th∆∞ m·ª•c g·ªëc: {os.path.abspath(output_dir_base)}")
print(f"   - ·∫¢nh ch·∫•p nh·∫≠n: {os.path.abspath(output_accepted_dir)}")
print(f"   - ·∫¢nh b·ªã lo·∫°i b·ªè: {os.path.abspath(output_rejected_dir)}")
print("===================================================================")