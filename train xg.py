# -*- coding: utf-8 -*-
import os
import pandas as pd
import numpy as np
from lime.lime_tabular import LimeTabularExplainer
from PIL import Image, UnidentifiedImageError
from tqdm import tqdm
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize
from sklearn.model_selection import train_test_split
from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,
                             roc_curve, auc, precision_recall_curve, average_precision_score)
import xgboost as xgb
import time
import warnings
import gc
import json
import matplotlib.pyplot as plt
import seaborn as sns
import shap

# Ghi l·∫°i th·ªùi gian b·∫Øt ƒë·∫ßu t·ªïng th·ªÉ c·ªßa script
script_start_time_main = time.time()

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

print("-------------------------------------------------------------")
print("--- Script K·∫øt h·ª£p Annotations v√† Ph√¢n lo·∫°i BI-RADS v5 (Objective softprob) ---")
print("-------------------------------------------------------------")

# =============================================================
# ===                1. THI·∫æT L·∫¨P C·∫§U H√åNH                 ===
# =============================================================
# --- ƒê∆∞·ªùng d·∫´n Annotations ---
breast_level_csv = 'them data/breast-level_annotations.csv'
finding_level_csv = 'them data/finding_annotations.csv'
# --- ƒê∆∞·ªùng d·∫´n ·∫¢nh ---
image_base_dir = 'orr'  # Th∆∞ m·ª•c CHA
train_dir_name = "train"
test_dir_name = "test"
IMAGE_EXTENSION = ".png"  # <<<--- !!! KI·ªÇM TRA V√Ä C·∫¨P NH·∫¨T ƒêU√îI FILE !!!
# --- ƒê∆∞·ªùng d·∫´n Output ---
output_dir = "classification_output_v001_softprob_lime_shap"  # Th∆∞ m·ª•c output
os.makedirs(output_dir, exist_ok=True)  # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
output_processed_csv_path = os.path.join(output_dir, "final_processed_annotations1.csv")
output_model_path = os.path.join(output_dir, "xgboost_birads_model_core_softprob1.json")
output_report_path = os.path.join(output_dir, "classification_results1.txt")
output_training_plot_path = os.path.join(output_dir, "training_performance_curve1.png")
output_cm_plot_path = os.path.join(output_dir, "confusion_matrix_heatmap1.png")
output_roc_plot_path = os.path.join(output_dir, "roc_curves_ovr1.png")
output_pr_plot_path = os.path.join(output_dir, "precision_recall_curves_ovr1.png")
# --- C·∫•u h√¨nh K·∫øt h·ª£p & Ph√¢n lo·∫°i ---
KEY_COLUMNS = ['image_id']
IMAGE_PATH_COL = 'image_path'
MODEL_NAME = 'resnet50'
FEATURE_VECTOR_SIZE = 2048
IMAGE_SIZE = (224, 224)
# Params cho xgb.train (API c·ªët l√µi) - S·ª¨ D·ª§NG SOFTPROB
XGB_PARAMS = {
    'max_depth': 7,
    'learning_rate': 0.1,
    'objective': 'multi:softprob',  # <<<--- ƒê·ªîI SANG SOFTPROB
    'eval_metric': 'mlogloss',
    'random_state': 42,
    'nthread': -1
}
NUM_BOOST_ROUND = 400  # S·ªë c√¢y t·ªëi ƒëa
EARLY_STOPPING_ROUNDS = 50  # S·ªë v√≤ng d·ª´ng s·ªõm
TEST_SIZE = 0.2
RANDOM_STATE = 42
METADATA_COLS_FOR_MODEL = ['view_position', 'breast_density', 'laterality']
TARGET_COL = 'breast_birads'
print(f"üìÇ Th∆∞ m·ª•c Output: {os.path.abspath(output_dir)}")

# =============================================================
# ===          2. ƒê·ªåC, L·ªåC V√Ä K·∫æT H·ª¢P ANNOTATIONS          ===
# =============================================================
print("\n[B∆∞·ªõc 1/10] ƒê·ªçc, l·ªçc v√† k·∫øt h·ª£p annotations...")
start_step_time = time.time()
# Ki·ªÉm tra file
if not os.path.exists(breast_level_csv):
    print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file '{breast_level_csv}'")
    exit()
if not os.path.exists(finding_level_csv):
    print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file '{finding_level_csv}'")
    exit()
print(f"   - T√¨m th·∫•y '{os.path.basename(breast_level_csv)}'.")
print(f"   - T√¨m th·∫•y '{os.path.basename(finding_level_csv)}'.")
# ƒê·ªçc file
try:
    df_breast = pd.read_csv(breast_level_csv)
    print(f"   - ƒê·ªçc {len(df_breast)} d√≤ng t·ª´ '{os.path.basename(breast_level_csv)}'.")
except Exception as e:
    print(f"‚ùå L·ªói khi ƒë·ªçc file CSV '{breast_level_csv}': {e}")
    exit()
try:
    df_finding = pd.read_csv(finding_level_csv)
    print(f"   - ƒê·ªçc {len(df_finding)} d√≤ng t·ª´ '{os.path.basename(finding_level_csv)}'.")
except Exception as e:
    print(f"‚ùå L·ªói khi ƒë·ªçc file CSV '{finding_level_csv}': {e}")
    exit()
# Ki·ªÉm tra c·ªôt kh√≥a
missing_keys_breast = [col for col in KEY_COLUMNS if col not in df_breast.columns]
missing_keys_finding = [col for col in KEY_COLUMNS if col not in df_finding.columns]
if missing_keys_breast:
    print(f"‚ùå L·ªói: File '{os.path.basename(breast_level_csv)}' thi·∫øu c·ªôt kh√≥a: {missing_keys_breast}")
    exit()
if missing_keys_finding:
    print(f"‚ùå L·ªói: File '{os.path.basename(finding_level_csv)}' thi·∫øu c·ªôt kh√≥a: {missing_keys_finding}")
    exit()
print(f"   - C·ªôt kh√≥a {KEY_COLUMNS} t·ªìn t·∫°i.")
# X√°c ƒë·ªãnh c·ªôt chung v√† l·ªçc
common_columns = list(set(df_breast.columns) & set(df_finding.columns))
print(f"   - X√°c ƒë·ªãnh {len(common_columns)} c·ªôt chung.")
df_breast_common = df_breast[common_columns].copy()
df_finding_common = df_finding[common_columns].copy()
# Merge
print(f"   - Th·ª±c hi·ªán 'inner' join tr√™n {KEY_COLUMNS}...")
merged_df = pd.merge(df_breast_common, df_finding_common, on=KEY_COLUMNS, how='inner', suffixes=('_breast', '_finding'))
# X·ª≠ l√Ω c·ªôt tr√πng t√™n (∆∞u ti√™n _breast)
processed_cols = set()
final_cols = []
for col in merged_df.columns:
    base_col = col.replace('_breast', '').replace('_finding', '')
    if base_col not in processed_cols:
        if f"{base_col}_breast" in merged_df.columns:
            merged_df[base_col] = merged_df[f"{base_col}_breast"]
        elif f"{base_col}_finding" in merged_df.columns:
            merged_df[base_col] = merged_df[f"{base_col}_finding"]
        elif base_col in merged_df.columns:
            pass
        else:
            continue
        final_cols.append(base_col)
        processed_cols.add(base_col)
merged_df = merged_df[final_cols]
print(f"   - S·ªë d√≤ng sau khi 'inner' join: {len(merged_df)}")
# X·ª≠ l√Ω tr√πng l·∫∑p
initial_merged_rows = len(merged_df)
merged_df.drop_duplicates(subset=KEY_COLUMNS, keep='first', inplace=True)
duplicates_dropped = initial_merged_rows - len(merged_df)
if duplicates_dropped > 0:
    print(f"   - ƒê√£ lo·∫°i b·ªè {duplicates_dropped} b·∫£n ghi tr√πng l·∫∑p.")
print(f"   - S·ªë d√≤ng cu·ªëi c√πng trong DataFrame k·∫øt h·ª£p: {len(merged_df)}")
if merged_df.empty:
    print("‚ùå L·ªói: Kh√¥ng c√≥ d·ªØ li·ªáu chung. K·∫øt th√∫c.")
    exit()
end_step_time = time.time()
print(f"‚úÖ K·∫øt h·ª£p annotations ho√†n t·∫•t (Th·ªùi gian: {end_step_time - start_step_time:.2f}s).")
del df_breast, df_finding, df_breast_common, df_finding_common  # Gi·∫£i ph√≥ng b·ªô nh·ªõ
gc.collect()

# =============================================================
# ===     3. T·∫†O ƒê∆Ø·ªúNG D·∫™N ·∫¢NH V√Ä KI·ªÇM TRA D·ªÆ LI·ªÜU        ===
# =============================================================
print("\n[B∆∞·ªõc 2/10] T·∫°o ƒë∆∞·ªùng d·∫´n ·∫£nh v√† ki·ªÉm tra d·ªØ li·ªáu k·∫øt h·ª£p...")
start_step_time = time.time()
# Ki·ªÉm tra c·ªôt c·∫ßn thi·∫øt
if 'split' not in merged_df.columns:
    print(f"‚ùå L·ªói: Thi·∫øu c·ªôt 'split'.")
    exit()
if 'image_id' not in merged_df.columns:
    print(f"‚ùå L·ªói: Thi·∫øu c·ªôt 'image_id'.")
    exit()
# H√†m t·∫°o ƒë∆∞·ªùng d·∫´n
def create_image_path(row):
    """T·∫°o ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi d·ª±a tr√™n split v√† image_id."""
    split_folder = train_dir_name if row['split'] == 'training' else test_dir_name
    image_filename = f"{row['image_id']}{IMAGE_EXTENSION}"
    path = os.path.join(split_folder, image_filename)
    return path
print(f"   - T·∫°o c·ªôt '{IMAGE_PATH_COL}' v·ªõi ƒëu√¥i file '{IMAGE_EXTENSION}'...")
merged_df[IMAGE_PATH_COL] = merged_df.apply(create_image_path, axis=1)
if merged_df[IMAGE_PATH_COL].isnull().any():
    print("   ‚ö†Ô∏è C·∫£nh b√°o: C√≥ l·ªói khi t·∫°o ƒë∆∞·ªùng d·∫´n ·∫£nh.")
    merged_df.dropna(subset=[IMAGE_PATH_COL], inplace=True)
print(f"     V√≠ d·ª• ƒë∆∞·ªùng d·∫´n: {merged_df[IMAGE_PATH_COL].iloc[0]}" if not merged_df.empty else "     (Kh√¥ng c√≥ d·ªØ li·ªáu)")
# Ki·ªÉm tra c·ªôt c·∫ßn thi·∫øt cho model
required_cols_model = [IMAGE_PATH_COL] + METADATA_COLS_FOR_MODEL + [TARGET_COL]
missing_cols_model = [col for col in required_cols_model if col not in merged_df.columns]
if missing_cols_model:
    print(f"‚ùå L·ªói: DataFrame thi·∫øu c·ªôt cho m√¥ h√¨nh: {missing_cols_model}")
    exit()
print(f"   - C√°c c·ªôt c·∫ßn thi·∫øt ({required_cols_model}) ƒë√£ t·ªìn t·∫°i.")
# X·ª≠ l√Ω NaN ch·ªâ trong c√°c c·ªôt breast_birads v√† image_id
required_cols_model = ['breast_birads', 'image_id']  # C√°c c·ªôt b·∫Øt bu·ªôc
initial_rows = len(merged_df)
merged_df.dropna(subset=required_cols_model, inplace=True)
dropped_rows = initial_rows - len(merged_df)
if dropped_rows > 0:
    print(f"   - ƒê√£ lo·∫°i b·ªè {dropped_rows} d√≤ng ch·ª©a gi√° tr·ªã thi·∫øu trong c√°c c·ªôt {required_cols_model}.")
print(f"   - S·ªë d√≤ng h·ª£p l·ªá sau khi lo·∫°i b·ªè NaN: {len(merged_df)}")
if merged_df.empty:
    print("‚ùå L·ªói: Kh√¥ng c√≤n d·ªØ li·ªáu h·ª£p l·ªá.")
    exit()
# M√£ h√≥a
print("   - M√£ h√≥a metadata v√† nh√£n...")
le_view = LabelEncoder()
le_laterality = LabelEncoder()
le_density = LabelEncoder()
le_birads = LabelEncoder()
try:
    merged_df['view_position_encoded'] = le_view.fit_transform(merged_df['view_position'])
    merged_df['laterality_encoded'] = le_laterality.fit_transform(merged_df['laterality'])
    if not pd.api.types.is_numeric_dtype(merged_df['breast_density']):
        merged_df['breast_density_encoded'] = le_density.fit_transform(merged_df['breast_density'])
    else:
        merged_df['breast_density_encoded'] = merged_df['breast_density']
    merged_df['birads_encoded'] = le_birads.fit_transform(merged_df[TARGET_COL])
    num_classes = len(le_birads.classes_)
    target_names = [str(cls) for cls in le_birads.classes_]
    XGB_PARAMS['num_class'] = num_classes  # C·∫≠p nh·∫≠t tham s·ªë XGBoost
    print(f"     - ƒê√£ m√£ h√≥a c√°c c·ªôt. S·ªë l·ªõp BI-RADS: {num_classes}")
except KeyError as ke:
    print(f"‚ùå L·ªói KeyError khi m√£ h√≥a c·ªôt: {ke}.")
    exit()
except Exception as e:
    print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh khi m√£ h√≥a: {e}")
    exit()
end_step_time = time.time()
print(f"‚úÖ Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu k·∫øt h·ª£p ho√†n t·∫•t (Th·ªùi gian: {end_step_time - start_step_time:.2f}s).")

# =============================================================
# ===          4. THI·∫æT L·∫¨P TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG           ===
# =============================================================
print("\n[B∆∞·ªõc 3/10] Thi·∫øt l·∫≠p m√¥ h√¨nh tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng ·∫£nh...")
start_step_time = time.time()
image_transforms = transforms.Compose([
    transforms.Resize(IMAGE_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"   - S·ª≠ d·ª•ng thi·∫øt b·ªã: {str(device).upper()}")
print(f"   - T·∫£i m√¥ h√¨nh {MODEL_NAME} pretrained...")
try:
    cnn_model = None
    if MODEL_NAME == 'resnet50':
        cnn_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)
        cnn_model.fc = nn.Identity()
    elif MODEL_NAME == 'resnet101':
        cnn_model = models.resnet101(weights=models.ResNet101_Weights.IMAGEN1K_V2)
        cnn_model.fc = nn.Identity()
    else:
        print(f"‚ùå L·ªói: Model '{MODEL_NAME}' kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£.")
        exit()
    if cnn_model:
        cnn_model = cnn_model.to(device)
        cnn_model.eval()
    else:
        print("‚ùå L·ªói: Kh√¥ng th·ªÉ kh·ªüi t·∫°o cnn_model.")
        exit()
    end_step_time = time.time()
    print(f"‚úÖ T·∫£i v√† c·∫•u h√¨nh {MODEL_NAME} th√†nh c√¥ng (Th·ªùi gian: {end_step_time - start_step_time:.2f}s).")
except Exception as e:
    print(f"‚ùå L·ªói khi t·∫£i/c·∫•u h√¨nh CNN: {e}")
    exit()

# =============================================================
# ===             5. TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG ·∫¢NH              ===
# =============================================================
augmented_images_dir = "augmented_images_xg" 
os.makedirs(augmented_images_dir, exist_ok=True)
LABEL_COL = 'birads_encoded'
print("\n[B∆∞·ªõc 4/10] B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng ·∫£nh (augment nh√£n 2,3,4 ƒë·ªÉ c√¢n b·∫±ng v·ªõi nh√£n 1)...")
start_step_time = time.time()
light_augment = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=5),
    transforms.ColorJitter(brightness=0.1, contrast=0.1),
    transforms.RandomAffine(degrees=3, translate=(0.02, 0.02), scale=(0.98, 1.02)),
    transforms.RandomResizedCrop(size=(224, 224), scale=(0.85, 1.0)),
])
strong_augment = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.7),
    transforms.RandomVerticalFlip(p=0.5),
    transforms.RandomRotation(degrees=15),
    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),
    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),
    transforms.RandomPerspective(distortion_scale=0.4, p=0.5),
    transforms.RandomResizedCrop(size=(224, 224), scale=(0.7, 1.0)),
])
def extract_feature_from_pil(img):
    img_tensor = image_transforms(img).unsqueeze(0).to(device)
    with torch.no_grad():
        feature_vector = cnn_model(img_tensor).squeeze().cpu().numpy()
    if np.isnan(feature_vector).any():
        return None
    return feature_vector
features_list = []
valid_indices_processed = []
augmentation_flags = []
skipped_files_count = 0
augment_image_count = 0
label_counts = merged_df[LABEL_COL].value_counts()
print(f"üîé S·ªë l∆∞·ª£ng ban ƒë·∫ßu: {dict(label_counts)}")
augment_labels = [2, 3, 4]
target_per_label =8000
current_augmented_counts = {label: label_counts.get(label, 0) for label in augment_labels}
print(f"üéØ M·ª•c ti√™u s·ªë l∆∞·ª£ng cho m·ªói nh√£n: {target_per_label}")
for idx, row in tqdm(merged_df.iterrows(), total=len(merged_df), desc="   Tr√≠ch xu·∫•t", unit="·∫£nh"):
    img_rel_path = row['image_path']
    img_full_path = os.path.join(image_base_dir, img_rel_path)
    label = row[LABEL_COL]
    try:
        img = Image.open(img_full_path).convert('RGB')
        feat = extract_feature_from_pil(img)
        if feat is not None and feat.shape == (FEATURE_VECTOR_SIZE,):
            features_list.append(feat)
            valid_indices_processed.append(idx)
            augmentation_flags.append(False)
            if label in augment_labels:
                while current_augmented_counts[label] < target_per_label:
                    aug = strong_augment if label == 4 else light_augment
                    aug_img = aug(img)
                    aug_feat = extract_feature_from_pil(aug_img)
                    if aug_feat is not None and aug_feat.shape == (FEATURE_VECTOR_SIZE,):
                        features_list.append(aug_feat)
                        valid_indices_processed.append(idx)
                        augmentation_flags.append(True)
                        augment_image_count += 1
                        current_augmented_counts[label] += 1
                        orig_filename = os.path.splitext(os.path.basename(img_rel_path))[0]
                        aug_filename = f"{orig_filename}_aug{augment_image_count}.jpg"
                        aug_save_path = os.path.join(augmented_images_dir, aug_filename)
                        aug_img.save(aug_save_path)
                    else:
                        break
        else:
            skipped_files_count += 1
    except (FileNotFoundError, UnidentifiedImageError):
        skipped_files_count += 1
    except Exception as e:
        skipped_files_count += 1
end_step_time = time.time()
print(f"‚úÖ Tr√≠ch xu·∫•t ho√†n t·∫•t (Th·ªùi gian: {end_step_time - start_step_time:.2f}s).")
print(f"   - S·ªë l∆∞·ª£ng ·∫£nh g·ªëc th√†nh c√¥ng: {len(merged_df) - skipped_files_count}")
print(f"   - S·ªë l∆∞·ª£ng ·∫£nh augment t·∫°o th√™m: {augment_image_count}")
print(f"   - T·ªïng s·ªë feature vector (g·ªëc + augment): {len(features_list)}")
print(f"   - S·ªë l∆∞·ª£ng m·ªõi sau augment: {current_augmented_counts}")
if skipped_files_count > 0:
    print(f"   - ·∫¢nh b·ªã l·ªói ho·∫∑c b·ªè qua: {skipped_files_count}")
if not features_list:
    print("‚ùå L·ªói: Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c b·∫•t k·ª≥ ƒë·∫∑c tr∆∞ng n√†o!")
    exit()
df_final = merged_df.loc[valid_indices_processed].reset_index(drop=True)
image_features = np.stack(features_list)
print(f"   - K√≠ch th∆∞·ªõc m·∫£ng ƒë·∫∑c tr∆∞ng ·∫£nh cu·ªëi c√πng: {image_features.shape}")
try:
    df_final.to_csv(output_processed_csv_path, index=False)
    print(f"üíæ ƒê√£ l∆∞u DataFrame ƒë√£ x·ª≠ l√Ω v√†o: {output_processed_csv_path}")
except Exception as e:
    print(f"‚ö†Ô∏è L·ªói khi l∆∞u file CSV ƒë√£ x·ª≠ l√Ω: {e}")
del merged_df, features_list
gc.collect()
torch.cuda.empty_cache() if device == 'cuda' else None

# =============================================================
# ===       6. K·∫æT H·ª¢P ƒê·∫∂C TR∆ØNG CU·ªêI & CHIA D·ªÆ LI·ªÜU       ===
# =============================================================
print("\n[B∆∞·ªõc 5/10] K·∫øt h·ª£p ƒë·∫∑c tr∆∞ng ·∫£nh v√† metadata cu·ªëi c√πng...")
start_step_time = time.time()
try:
    metadata_features_final = df_final[['view_position_encoded', 'breast_density_encoded', 'laterality_encoded']].values
    scaler = StandardScaler()
    metadata_scaled_final = scaler.fit_transform(metadata_features_final)
    print("   - ƒê√£ √°p d·ª•ng StandardScaler cho metadata.")
    X = np.hstack([image_features, metadata_scaled_final])
    y = df_final['birads_encoded'].values
    print(f"   - K√≠ch th∆∞·ªõc m·∫£ng ƒë·∫∑c tr∆∞ng k·∫øt h·ª£p (X): {X.shape}")
    print(f"   - K√≠ch th∆∞·ªõc m·∫£ng nh√£n (y): {y.shape}")
except KeyError as ke:
    print(f"‚ùå L·ªói KeyError khi l·∫•y c·ªôt metadata: {ke}. Ki·ªÉm tra l·∫°i METADATA_COLS_FOR_MODEL.")
    exit()
except Exception as e:
    print(f"‚ùå L·ªói khi k·∫øt h·ª£p ƒë·∫∑c tr∆∞ng: {e}")
    exit()
end_step_time = time.time()
print(f"‚úÖ K·∫øt h·ª£p ƒë·∫∑c tr∆∞ng ho√†n t·∫•t (Th·ªùi gian: {end_step_time - start_step_time:.2f}s).")
print(f"\n[B∆∞·ªõc 6/10] Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra...")
start_step_time = time.time()
try:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE)
    print(f"   - T·∫≠p hu·∫•n luy·ªán: X={X_train.shape}, y={y_train.shape}")
    print(f"   - T·∫≠p ki·ªÉm tra:  X={X_test.shape}, y={y_test.shape}")
except Exception as e:
    print(f"‚ùå L·ªói khi chia d·ªØ li·ªáu: {e}")
    exit()
end_step_time = time.time()
print(f"‚úÖ Chia d·ªØ li·ªáu ho√†n t·∫•t (Th·ªùi gian: {end_step_time - start_step_time:.2f}s).")
print("   - T·∫°o DMatrix cho XGBoost...")
try:
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dtest = xgb.DMatrix(X_test, label=y_test)
    watchlist = [(dtrain, 'train'), (dtest, 'test')]
    print("   - T·∫°o DMatrix th√†nh c√¥ng.")
except Exception as e:
    print(f"‚ùå L·ªói khi t·∫°o DMatrix: {e}")
    exit()

# =============================================================
# ===            7. HU·∫§N LUY·ªÜN V√Ä L∆ØU M√î H√åNH              ===
# =============================================================
print("\n[B∆∞·ªõc 7/10] Hu·∫•n luy·ªán m√¥ h√¨nh XGBoost (Core API) v√† L∆∞u m√¥ h√¨nh...")
start_step_time = time.time()
print("   - Chu·∫©n b·ªã tham s·ªë v√† hu·∫•n luy·ªán b·∫±ng xgb.train...")
params = XGB_PARAMS.copy()  # ƒê√£ c·∫≠p nh·∫≠t num_class ·ªü B∆∞·ªõc 2
evals_result = {}
bst = None  # Kh·ªüi t·∫°o bst
try:
    bst = xgb.train(
        params,
        dtrain,
        num_boost_round=NUM_BOOST_ROUND,
        evals=watchlist,
        early_stopping_rounds=EARLY_STOPPING_ROUNDS,
        evals_result=evals_result,
        verbose_eval=False
    )
except xgb.core.XGBoostError as xgb_err:
    print(f"‚ùå L·ªói XGBoost khi hu·∫•n luy·ªán: {xgb_err}")
    exit()
except Exception as e:
    print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh khi hu·∫•n luy·ªán: {e}")
    exit()
end_step_time = time.time()
print(f"‚úÖ Hu·∫•n luy·ªán XGBoost ho√†n t·∫•t (Th·ªùi gian: {end_step_time - start_step_time:.2f}s).")
# L·∫•y s·ªë iteration t·ªët nh·∫•t m·ªôt c√°ch an to√†n
best_iteration = getattr(bst, 'best_iteration', -1)  # D√πng getattr ƒë·ªÉ tr√°nh AttributeError
optimal_num_trees = NUM_BOOST_ROUND
if best_iteration >= 0:
    optimal_num_trees = best_iteration + 1
    print(f"   - S·ªë l∆∞·ª£ng c√¢y t·ªëi ∆∞u (best_iteration + 1): {optimal_num_trees} / {NUM_BOOST_ROUND}")
else:
    print(f"   - Early stopping kh√¥ng k√≠ch ho·∫°t/l·ªói, d√πng {NUM_BOOST_ROUND} c√¢y.")
# V·∫Ω Bi·ªÉu ƒë·ªì Hu·∫•n luy·ªán
print(f"   - V·∫Ω v√† l∆∞u bi·ªÉu ƒë·ªì hi·ªáu su·∫•t hu·∫•n luy·ªán: {output_training_plot_path}")
fig_train = None
try:
    metric_name = params['eval_metric']
    if evals_result and 'train' in evals_result and 'test' in evals_result and \
       metric_name in evals_result['train'] and metric_name in evals_result['test']:
        train_metric_history = evals_result['train'][metric_name]
        test_metric_history = evals_result['test'][metric_name]
        epochs = len(train_metric_history)
        x_axis = range(0, epochs)
        fig_train, ax = plt.subplots(figsize=(10, 6))
        ax.plot(x_axis, train_metric_history, label=f'Train {metric_name}')
        ax.plot(x_axis, test_metric_history, label=f'Test {metric_name}')
        if best_iteration >= 0:
            ax.axvline(best_iteration, color='r', linestyle='--', label=f'Best Iteration ({optimal_num_trees})')
        ax.legend()
        plt.ylabel(metric_name.capitalize())
        plt.xlabel('Boosting Iterations (Trees)')
        plt.title(f'XGBoost {metric_name.capitalize()} during Training')
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(output_training_plot_path)
        print("     üíæ Bi·ªÉu ƒë·ªì hu·∫•n luy·ªán ƒë√£ l∆∞u.")
    else:
        print(f"     ‚ö†Ô∏è Kh√¥ng ƒë·ªß d·ªØ li·ªáu trong evals_result cho metric '{metric_name}' ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì.")
except KeyError as ke:
    print(f"     ‚ö†Ô∏è L·ªói KeyError khi truy c·∫≠p evals_result (metric '{metric_name}'): {ke}")
except Exception as e:
    print(f"     ‚ö†Ô∏è L·ªói khi v·∫Ω/l∆∞u bi·ªÉu ƒë·ªì hu·∫•n luy·ªán: {e}")
finally:
    plt.close(fig_train) if fig_train else None
# L∆∞u Model
print(f"   - L∆∞u m√¥ h√¨nh (Booster) v√†o: {output_model_path}")
try:
    bst.save_model(output_model_path)
    print("     üíæ L∆∞u m√¥ h√¨nh th√†nh c√¥ng.")
except Exception as e:
    print(f"     ‚ö†Ô∏è L·ªói khi l∆∞u m√¥ h√¨nh Booster: {e}")

# =============================================================
# ===          8. ƒê√ÅNH GI√Å CHI TI·∫æT M√î H√åNH                ===
# =============================================================
print("\n[B∆∞·ªõc 8/10] ƒê√°nh gi√° chi ti·∫øt m√¥ h√¨nh...")
start_step_time = time.time()
# D·ª± ƒëo√°n b·∫±ng Booster
y_pred = None
y_pred_proba = None
metrics_calculated = False
if bst is None:  # Ki·ªÉm tra n·∫øu hu·∫•n luy·ªán th·∫•t b·∫°i
    print("‚ùå L·ªói: M√¥ h√¨nh (bst) kh√¥ng ƒë∆∞·ª£c hu·∫•n luy·ªán th√†nh c√¥ng, kh√¥ng th·ªÉ ƒë√°nh gi√°.")
else:
    try:
        limit = optimal_num_trees  # D√πng s·ªë c√¢y ƒë√£ x√°c ƒë·ªãnh
        print(f"   - D·ª± ƒëo√°n x√°c su·∫•t b·∫±ng Booster v·ªõi gi·ªõi h·∫°n c√¢y: {limit}")
        y_pred_proba = bst.predict(dtest, iteration_range=(0, limit))  # D√πng iteration_range thay v√¨ ntree_limit
        if y_pred_proba is not None and y_pred_proba.ndim == 2 and y_pred_proba.shape[0] == dtest.num_row() and y_pred_proba.shape[1] == num_classes:
            y_pred = np.argmax(y_pred_proba, axis=1)
            print("   - ƒê√£ l·∫•y nh√£n l·ªõp d·ª± ƒëo√°n t·ª´ x√°c su·∫•t.")
            metrics_calculated = True
        else:
            print(f"   ‚ùå L·ªói: Output d·ª± ƒëo√°n x√°c su·∫•t c√≥ shape kh√¥ng mong mu·ªën: {y_pred_proba.shape if y_pred_proba is not None else 'None'}")
            print(f"     (K·ª≥ v·ªçng: ({dtest.num_row()}, {num_classes}))")
    except xgb.core.XGBoostError as pred_e:
        print(f"‚ùå L·ªói XGBoost khi d·ª± ƒëo√°n: {pred_e}")
    except Exception as pred_e:
        print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh khi d·ª± ƒëo√°n: {pred_e}")
# T√≠nh Metrics n·∫øu d·ª± ƒëo√°n th√†nh c√¥ng
accuracy = -1.0
report_str = "L·ªói d·ª± ƒëo√°n/l·∫•y nh√£n"
cm = np.array([[-1]])
cm_df = pd.DataFrame()
if metrics_calculated:
    try:
        y_true_labels = dtest.get_label().astype(int)
        accuracy = accuracy_score(y_true_labels, y_pred)
        report_str = classification_report(y_true_labels, y_pred, target_names=target_names, zero_division=0)
        cm = confusion_matrix(y_true_labels, y_pred, labels=range(num_classes))
        cm_df = pd.DataFrame(cm, index=le_birads.classes_, columns=le_birads.classes_)
    except Exception as metric_e:
        print(f"‚ùå L·ªói khi t√≠nh to√°n metrics: {metric_e}")
        metrics_calculated = False
        accuracy = -1.0
        report_str = "L·ªói khi t√≠nh metrics."
        cm_df = pd.DataFrame()
# In K·∫øt qu·∫£
print(f"\n   --- K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n t·∫≠p ki·ªÉm tra ---")
print(f"   - ƒê·ªô ch√≠nh x√°c (Accuracy): {accuracy:.4f}" if metrics_calculated else "   - ƒê·ªô ch√≠nh x√°c (Accuracy): L·ªói")
if metrics_calculated:
    print("\n   - B√°o c√°o ph√¢n lo·∫°i chi ti·∫øt (Classification Report):")
    print(report_str)
    print("\n   - Ma tr·∫≠n nh·∫ßm l·∫´n (Confusion Matrix):")
    print(cm_df)
else:
    print("\n   - Kh√¥ng c√≥ b√°o c√°o v√† ma tr·∫≠n nh·∫ßm l·∫´n do l·ªói.")
# V·∫Ω Bi·ªÉu ƒë·ªì ƒê√°nh gi√° ch·ªâ khi metrics ƒë∆∞·ª£c t√≠nh V√Ä c√≥ x√°c su·∫•t
plot_evaluation_charts = metrics_calculated and (y_pred_proba is not None)
if plot_evaluation_charts:
    # Heatmap CM
    print(f"\n   - V·∫Ω v√† l∆∞u heatmap ma tr·∫≠n nh·∫ßm l·∫´n: {output_cm_plot_path}")
    fig_cm = None
    try:
        fig_cm, ax_cm = plt.subplots(figsize=(8, 6))
        sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', ax=ax_cm, annot_kws={"size": 12})
        ax_cm.set_xlabel('Nh√£n D·ª± ƒëo√°n', fontsize=12)
        ax_cm.set_ylabel('Nh√£n Th·ª±c', fontsize=12)
        ax_cm.set_title('Heatmap Ma tr·∫≠n Nh·∫ßm l·∫´n', fontsize=14)
        ax_cm.tick_params(axis='both', which='major', labelsize=10)
        plt.tight_layout()
        plt.savefig(output_cm_plot_path)
        print("     üíæ Heatmap ƒë√£ l∆∞u.")
    except Exception as e:
        print(f"     ‚ö†Ô∏è L·ªói khi v·∫Ω/l∆∞u heatmap: {e}")
    finally:
        plt.close(fig_cm) if fig_cm else None
    # ROC Curve
    print(f"\n   - V·∫Ω v√† l∆∞u ƒë∆∞·ªùng cong ROC (One-vs-Rest): {output_roc_plot_path}")
    fig_roc = None
    try:
        y_test_binarized = label_binarize(y_true_labels, classes=range(num_classes))
        assert y_test_binarized.shape[1] >= 2, "C·∫ßn √≠t nh·∫•t 2 l·ªõp ƒë·ªÉ v·∫Ω ROC."
        fpr = dict()
        tpr = dict()
        roc_auc = dict()
        fig_roc, ax_roc = plt.subplots(figsize=(10, 7))
        for i in range(num_classes):
            fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_proba[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
            ax_roc.plot(fpr[i], tpr[i], lw=2, label=f'ROC l·ªõp {target_names[i]} (AUC = {roc_auc[i]:.2f})')
        ax_roc.plot([0, 1], [0, 1], 'k--', lw=2, label='Ng·∫´u nhi√™n')
        ax_roc.set_xlim([0.0, 1.0])
        ax_roc.set_ylim([0.0, 1.05])
        ax_roc.set_xlabel('T·ª∑ l·ªá D∆∞∆°ng t√≠nh Gi·∫£')
        ax_roc.set_ylabel('T·ª∑ l·ªá D∆∞∆°ng t√≠nh Th·ª±c')
        ax_roc.set_title('ƒê∆∞·ªùng cong ROC - One-vs-Rest')
        ax_roc.legend(loc="lower right")
        ax_roc.grid(True)
        plt.tight_layout()
        plt.savefig(output_roc_plot_path)
        print("     üíæ Bi·ªÉu ƒë·ªì ROC ƒë√£ l∆∞u.")
    except AssertionError as ae:
        print(f"     ‚ö†Ô∏è L·ªói khi v·∫Ω ROC: {ae}")
    except ValueError as ve:
        print(f"     ‚ö†Ô∏è L·ªói ValueError khi v·∫Ω ROC: {ve}")
    except Exception as e:
        print(f"     ‚ö†Ô∏è L·ªói kh√¥ng x√°c ƒë·ªãnh khi v·∫Ω/l∆∞u ROC: {e}")
    finally:
        plt.close(fig_roc) if fig_roc else None
    # PR Curve
    print(f"\n   - V·∫Ω v√† l∆∞u ƒë∆∞·ªùng cong Precision-Recall (One-vs-Rest): {output_pr_plot_path}")
    fig_pr = None
    try:
        assert y_test_binarized.shape[1] >= 2, "C·∫ßn √≠t nh·∫•t 2 l·ªõp ƒë·ªÉ v·∫Ω PR."
        precision = dict()
        recall = dict()
        average_precision = dict()
        fig_pr, ax_pr = plt.subplots(figsize=(10, 7))
        for i in range(num_classes):
            precision[i], recall[i], _ = precision_recall_curve(y_test_binarized[:, i], y_pred_proba[:, i])
            average_precision[i] = average_precision_score(y_test_binarized[:, i], y_pred_proba[:, i])
            ax_pr.plot(recall[i], precision[i], lw=2, label=f'PR l·ªõp {target_names[i]} (AP = {average_precision[i]:.2f})')
        ax_pr.set_xlim([0.0, 1.0])
        ax_pr.set_ylim([0.0, 1.05])
        ax_pr.set_xlabel('ƒê·ªô Nh·ªõ l·∫°i (Recall)')
        ax_pr.set_ylabel('ƒê·ªô Ch√≠nh x√°c (Precision)')
        ax_pr.set_title('ƒê∆∞·ªùng cong Precision-Recall - One-vs-Rest')
        ax_pr.legend(loc="best")
        ax_pr.grid(True)
        plt.tight_layout()
        plt.savefig(output_pr_plot_path)
        print("     üíæ Bi·ªÉu ƒë·ªì PR ƒë√£ l∆∞u.")
    except AssertionError as ae:
        print(f"     ‚ö†Ô∏è L·ªói khi v·∫Ω PR: {ae}")
    except ValueError as ve:
        print(f"     ‚ö†Ô∏è L·ªói ValueError khi v·∫Ω PR: {ve}")
    except Exception as e:
        print(f"     ‚ö†Ô∏è L·ªói kh√¥ng x√°c ƒë·ªãnh khi v·∫Ω/l∆∞u PR: {e}")
    finally:
        plt.close(fig_pr) if fig_pr else None
else:
    print("\n   ‚ö†Ô∏è B·ªè qua v·∫Ω bi·ªÉu ƒë·ªì ƒë√°nh gi√° do l·ªói ho·∫∑c thi·∫øu x√°c su·∫•t.")
end_step_time = time.time()
print(f"‚úÖ ƒê√°nh gi√° chi ti·∫øt ho√†n t·∫•t (Th·ªùi gian: {end_step_time - start_step_time:.2f}s).")

# =============================================================
# ===            9. GI·∫¢I TH√çCH LIME                        ===
# =============================================================
print("\n[B∆∞·ªõc 9/10] Gi·∫£i th√≠ch m√¥ h√¨nh v·ªõi LIME...")
start_step_time = time.time()
lime_plot_paths = []
try:
    # H√†m d·ª± ƒëo√°n cho LIME, t∆∞∆°ng th√≠ch v·ªõi Booster
    def predict_fn(X):
        dmatrix = xgb.DMatrix(X)
        return bst.predict(dmatrix, iteration_range=(0, optimal_num_trees))
    explainer = LimeTabularExplainer(
        training_data=X_train,
        feature_names=[f"img_f{i}" for i in range(FEATURE_VECTOR_SIZE)] + ['view_position', 'breast_density', 'laterality'],
        class_names=target_names,
        mode="classification"
    )
    target_num_samples = 10
    samples_per_label = target_num_samples // num_classes
    selected_indices = []
    selected_labels = []
    for label in range(num_classes):
        label_indices = np.where(y_test == label)[0]
        if len(label_indices) == 0:
            print(f"     ‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng c√≥ m·∫´u n√†o cho nh√£n {target_names[label]}.")
            continue
        num_to_select = min(samples_per_label, len(label_indices))
        selected = np.random.choice(label_indices, size=num_to_select, replace=False)
        selected_indices.extend(selected)
        selected_labels.extend([label] * num_to_select)
    current_num_samples = len(selected_indices)
    if current_num_samples < target_num_samples:
        remaining_needed = target_num_samples - current_num_samples
        all_indices = np.arange(len(y_test))
        remaining_indices = np.setdiff1d(all_indices, selected_indices)
        if len(remaining_indices) < remaining_needed:
            print(f"     ‚ö†Ô∏è C·∫£nh b√°o: Ch·ªâ c√≥ {len(remaining_indices)} m·∫´u c√≤n l·∫°i, kh√¥ng ƒë·ªß ƒë·ªÉ ch·ªçn {remaining_needed} m·∫´u.")
            selected_indices.extend(remaining_indices)
            selected_labels.extend(y_test[remaining_indices].tolist())
        else:
            extra_selected = np.random.choice(remaining_indices, size=remaining_needed, replace=False)
            selected_indices.extend(extra_selected)
            selected_labels.extend(y_test[extra_selected].tolist())
    print(f"   - S·ªë m·∫´u ƒë∆∞·ª£c ch·ªçn cho LIME: {len(selected_indices)} (m·ª•c ti√™u: {target_num_samples})")
    print(f"   - Ph√¢n b·ªë nh√£n ƒë∆∞·ª£c ch·ªçn: {dict(pd.Series(selected_labels).value_counts())}")
    for i, sample_idx in enumerate(selected_indices[:target_num_samples]):
        sample = X_test[sample_idx]
        true_label = target_names[y_test[sample_idx]]
        explanation = explainer.explain_instance(
            data_row=sample,
            predict_fn=predict_fn,
            num_features=10
        )
        print(f"   - Gi·∫£i th√≠ch LIME cho m·∫´u {i} - Nh√£n: {true_label}")
        fig = explanation.as_pyplot_figure()
        plt.title(f"Gi·∫£i th√≠ch LIME - M·∫´u {i} - Nh√£n: {true_label}")
        plt.tight_layout()
        lime_plot_path = os.path.join(output_dir, f"lime_explanation_label{selected_labels[i]}_sample{i}.png")
        plt.savefig(lime_plot_path)
        plt.close(fig)
        lime_plot_paths.append(lime_plot_path)
        print(f"     üíæ L∆∞u bi·ªÉu ƒë·ªì LIME: {lime_plot_path}")
except Exception as e:
    print(f"‚ùå L·ªói khi t·∫°o gi·∫£i th√≠ch LIME: {e}")
end_step_time = time.time()
print(f"‚úÖ Gi·∫£i th√≠ch LIME ho√†n t·∫•t (Th·ªùi gian: {end_step_time - start_step_time:.2f}s).")

# =============================================================
# ===            10. GI·∫¢I TH√çCH SHAP                       ===
# =============================================================
print("\n[B∆∞·ªõc 10/10] Gi·∫£i th√≠ch m√¥ h√¨nh v·ªõi SHAP...")
plt.switch_backend('agg')
start_step_time = time.time()
shap_plot_path = ""
shap_waterfall_paths = []
try:
    print("   - Kh·ªüi t·∫°o TreeExplainer v·ªõi feature_perturbation='interventional'...")
    explainer = shap.TreeExplainer(
        bst,
        feature_perturbation='interventional',
        data=X_train[:100]  # S·ª≠ d·ª•ng t·∫≠p n·ªÅn nh·ªè
    )
    shap_values = explainer.shap_values(X_test, check_additivity=False)
    print(f"   - K√≠ch th∆∞·ªõc X_test: {X_test.shape}")
    print(f"   - K√≠ch th∆∞·ªõc y_test: {y_test.shape}")
    if len(X_test) != len(y_test):
        raise ValueError(f"K√≠ch th∆∞·ªõc kh√¥ng kh·ªõp: X_test ({len(X_test)}) ‚â† y_test ({len(y_test)})")
    # G·ª° l·ªói: Ki·ªÉm tra ƒë·∫ßu ra m√¥ h√¨nh so v·ªõi t·ªïng gi√° tr·ªã SHAP
    sample_idx = 0
    dmatrix_sample = xgb.DMatrix(X_test[sample_idx:sample_idx+1])
    model_output = bst.predict(dmatrix_sample, iteration_range=(0, optimal_num_trees))[0]
    shap_sum = np.sum(shap_values, axis=1)[sample_idx]
    print(f"   - G·ª° l·ªói (m·∫´u 0): ƒê·∫ßu ra m√¥ h√¨nh = {model_output}, T·ªïng gi√° tr·ªã SHAP = {shap_sum}")
    plt.figure(figsize=(12, 8))
    shap.summary_plot(
        shap_values,
        X_test,
        feature_names=[f"img_f{i}" for i in range(FEATURE_VECTOR_SIZE)] + ['view_position', 'breast_density', 'laterality'],
        plot_type="bar",
        class_names=target_names,
        color_bar=False,
        max_display=20,
        show=False
    )
    plt.title(f"Bi·ªÉu ƒë·ªì T√≥m t·∫Øt SHAP cho M√¥ h√¨nh XGBoost ({len(X_test)} M·∫´u Ki·ªÉm tra)", fontsize=14, pad=20)
    plt.xlabel("Trung b√¨nh(|Gi√° tr·ªã SHAP|) [t√°c ƒë·ªông trung b√¨nh ƒë·∫øn ƒë·∫ßu ra m√¥ h√¨nh]", fontsize=12)
    plt.ylabel("ƒê·∫∑c tr∆∞ng", fontsize=12)
    plt.tight_layout()
    shap_plot_path = os.path.join(output_dir, "shap_summary_model.png")
    plt.savefig(shap_plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"     üíæ L∆∞u bi·ªÉu ƒë·ªì SHAP t√≥m t·∫Øt to√†n b·ªô m√¥ h√¨nh: {shap_plot_path}")
    target_num_samples = 5
    valid_range = np.arange(min(2050, len(X_test)))
    selected_indices = np.random.choice(valid_range, size=target_num_samples, replace=False)
    selected_indices = [i for i in selected_indices if i < len(X_test)]
    if not selected_indices:
        raise ValueError("Kh√¥ng c√≥ ch·ªâ s·ªë h·ª£p l·ªá n√†o ƒë∆∞·ª£c ch·ªçn cho bi·ªÉu ƒë·ªì SHAP waterfall.")
    print(f"   - S·ªë m·∫´u ƒë∆∞·ª£c ch·ªçn cho SHAP waterfall: {len(selected_indices)} (ch·ªâ s·ªë: {selected_indices})")
    for i, sample_idx in enumerate(selected_indices):
        if sample_idx >= len(X_test):
            print(f"     ‚ö†Ô∏è C·∫£nh b√°o: Ch·ªâ s·ªë {sample_idx} v∆∞·ª£t qu√° k√≠ch th∆∞·ªõc X_test ({len(X_test)}). B·ªè qua m·∫´u n√†y.")
            continue
        true_label = target_names[y_test[sample_idx]]
        predicted_label = target_names[y_pred[sample_idx]] if y_pred is not None else "N/A"
        pred_class = y_pred[sample_idx] if y_pred is not None else np.argmax(bst.predict(xgb.DMatrix(X_test[sample_idx:sample_idx+1]), iteration_range=(0, optimal_num_trees))[0])
        shap_explanation = shap.Explanation(
            values=shap_values[pred_class][sample_idx],
            base_values=explainer.expected_value[pred_class],
            data=X_test[sample_idx],
            feature_names=[f"img_f{i}" for i in range(FEATURE_VECTOR_SIZE)] + ['view_position', 'breast_density', 'laterality']
        )
        plt.figure(figsize=(12, 6))
        shap.plots.waterfall(
            shap_explanation,
            max_display=10,
            show=False
        )
        plt.title(f"Bi·ªÉu ƒë·ªì SHAP Waterfall - M·∫´u {i} - Th·ª±c: {true_label}, D·ª± ƒëo√°n: {predicted_label}", fontsize=12, pad=10)
        plt.xlabel("Gi√° tr·ªã SHAP", fontsize=10)
        plt.ylabel("ƒê·∫∑c tr∆∞ng", fontsize=10)
        plt.tight_layout()
        waterfall_plot_path = os.path.join(output_dir, f"shap_waterfall_sample{i}.png")
        plt.savefig(waterfall_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        shap_waterfall_paths.append(waterfall_plot_path)
        print(f"     üíæ L∆∞u bi·ªÉu ƒë·ªì SHAP waterfall cho m·∫´u {i}: {waterfall_plot_path}")
except Exception as tree_e:
    print(f"‚ùå L·ªói TreeExplainer: {tree_e}")
    print("   - Chuy·ªÉn sang d√πng KernelExplainer l√†m ph∆∞∆°ng √°n d·ª± ph√≤ng...")
    try:
        background_size = min(100, X_train.shape[0])
        background_indices = np.random.choice(X_train.shape[0], size=background_size, replace=False)
        X_background = X_train[background_indices]
        def predict_fn_kernel(X):
            dmatrix = xgb.DMatrix(X)
            return bst.predict(dmatrix, iteration_range=(0, optimal_num_trees))
        explainer = shap.KernelExplainer(predict_fn_kernel, X_background)
        shap_values = explainer.shap_values(X_test, nsamples=100)
        print(f"   - K√≠ch th∆∞·ªõc X_test: {X_test.shape}")
        print(f"   - K√≠ch th∆∞·ªõc y_test: {y_test.shape}")
        if len(X_test) != len(y_test):
            raise ValueError(f"K√≠ch th∆∞·ªõc kh√¥ng kh·ªõp: X_test ({len(X_test)}) ‚â† y_test ({len(y_test)})")
        sample_idx = 0
        dmatrix_sample = xgb.DMatrix(X_test[sample_idx:sample_idx+1])
        model_output = bst.predict(dmatrix_sample, iteration_range=(0, optimal_num_trees))[0]
        shap_sum = np.sum(shap_values, axis=1)[sample_idx]
        print(f"   - G·ª° l·ªói (m·∫´u 0): ƒê·∫ßu ra m√¥ h√¨nh = {model_output}, T·ªïng gi√° tr·ªã SHAP = {shap_sum}")
        plt.figure(figsize=(12, 8))
        shap.summary_plot(
            shap_values,
            X_test,
            feature_names=[f"img_f{i}" for i in range(FEATURE_VECTOR_SIZE)] + ['view_position', 'breast_density', 'laterality'],
            plot_type="bar",
            class_names=target_names,
            color_bar=False,
            max_display=20,
            show=False
        )
        plt.title(f"Bi·ªÉu ƒë·ªì T√≥m t·∫Øt SHAP cho M√¥ h√¨nh XGBoost ({len(X_test)} M·∫´u Ki·ªÉm tra)", fontsize=14, pad=20)
        plt.xlabel("Trung b√¨nh(|Gi√° tr·ªã SHAP|) [t√°c ƒë·ªông trung b√¨nh ƒë·∫øn ƒë·∫ßu ra m√¥ h√¨nh]", fontsize=12)
        plt.ylabel("ƒê·∫∑c tr∆∞ng", fontsize=12)
        plt.tight_layout()
        shap_plot_path = os.path.join(output_dir, "shap_summary_model_kernel.png")
        plt.savefig(shap_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"     üíæ L∆∞u bi·ªÉu ƒë·ªì SHAP t√≥m t·∫Øt to√†n b·ªô m√¥ h√¨nh (KernelExplainer): {shap_plot_path}")
        target_num_samples = 5
        valid_range = np.arange(min(2050, len(X_test)))
        selected_indices = np.random.choice(valid_range, size=target_num_samples, replace=False)
        selected_indices = [i for i in selected_indices if i < len(X_test)]
        if not selected_indices:
            raise ValueError("Kh√¥ng c√≥ ch·ªâ s·ªë h·ª£p l·ªá n√†o ƒë∆∞·ª£c ch·ªçn cho bi·ªÉu ƒë·ªì SHAP waterfall.")
        print(f"   - S·ªë m·∫´u ƒë∆∞·ª£c ch·ªçn cho SHAP waterfall: {len(selected_indices)} (ch·ªâ s·ªë: {selected_indices})")
        for i, sample_idx in enumerate(selected_indices):
            if sample_idx >= len(X_test):
                print(f"     ‚ö†Ô∏è C·∫£nh b√°o: Ch·ªâ s·ªë {sample_idx} v∆∞·ª£t qu√° k√≠ch th∆∞·ªõc X_test ({len(X_test)}). B·ªè qua m·∫´u n√†y.")
                continue
            true_label = target_names[y_test[sample_idx]]
            predicted_label = target_names[y_pred[sample_idx]] if y_pred is not None else "N/A"
            pred_class = y_pred[sample_idx] if y_pred is not None else np.argmax(bst.predict(xgb.DMatrix(X_test[sample_idx:sample_idx+1]), iteration_range=(0, optimal_num_trees))[0])
            shap_explanation = shap.Explanation(
                values=shap_values[pred_class][sample_idx],
                base_values=explainer.expected_value[pred_class],
                data=X_test[sample_idx],
                feature_names=[f"img_f{i}" for i in range(FEATURE_VECTOR_SIZE)] + ['view_position', 'breast_density', 'laterality']
            )
            plt.figure(figsize=(12, 6))
            shap.plots.waterfall(
                shap_explanation,
                max_display=10,
                show=False
            )
            plt.title(f"Bi·ªÉu ƒë·ªì SHAP Waterfall - M·∫´u {i} - Th·ª±c: {true_label}, D·ª± ƒëo√°n: {predicted_label}", fontsize=12, pad=10)
            plt.xlabel("Gi√° tr·ªã SHAP", fontsize=10)
            plt.ylabel("ƒê·∫∑c tr∆∞ng", fontsize=10)
            plt.tight_layout()
            waterfall_plot_path = os.path.join(output_dir, f"shap_waterfall_sample{i}_kernel.png")
            plt.savefig(waterfall_plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            shap_waterfall_paths.append(waterfall_plot_path)
            print(f"     üíæ L∆∞u bi·ªÉu ƒë·ªì SHAP waterfall cho m·∫´u {i} (KernelExplainer): {waterfall_plot_path}")
    except Exception as kernel_e:
        print(f"‚ùå L·ªói KernelExplainer: {kernel_e}")
        print("   - B·ªè qua SHAP do l·ªói li√™n t·ª•c. Vui l√≤ng ki·ªÉm tra d·ªØ li·ªáu ho·∫∑c b√°o c√°o v·∫•n ƒë·ªÅ tr√™n GitHub SHAP.")
end_step_time = time.time()
print(f"‚úÖ Gi·∫£i th√≠ch SHAP ho√†n t·∫•t (Th·ªùi gian: {end_step_time - start_step_time:.2f}s).")

# =============================================================
# ===          L∆ØU K·∫æT QU·∫¢ TEXT                           ===
# =============================================================
print("\n[L∆∞u K·∫øt Qu·∫£] L∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° v√†o file text...")
start_step_time = time.time()
try:
    script_end_time = time.time()
    total_script_duration = script_end_time - script_start_time_main
    with open(output_report_path, 'w', encoding='utf-8') as f:
        f.write("="*53 + "\n")
        f.write("===       K·∫æT QU·∫¢ ƒê√ÅNH GI√Å M√î H√åNH PH√ÇN LO·∫†I BI-RADS       ===\n")
        f.write("="*53 + "\n\n")
        f.write(f"Th·ªùi gian ch·∫°y t·ªïng c·ªông script: {total_script_duration:.2f} gi√¢y ({total_script_duration/60:.2f} ph√∫t)\n\n")
        f.write(f"C·∫•u h√¨nh ch√≠nh:\n")
        f.write(f"  - Model CNN: {MODEL_NAME}\n")
        f.write(f"  - K√≠ch th∆∞·ªõc ·∫£nh: {IMAGE_SIZE}\n")
        f.write(f"  - Tham s·ªë XGBoost Core API: {json.dumps(params, indent=4)}\n")
        f.write(f"  - S·ªë c√¢y t·ªëi ƒëa (num_boost_round): {NUM_BOOST_ROUND}\n")
        f.write(f"  - Early Stopping Rounds: {EARLY_STOPPING_ROUNDS}\n")
        f.write(f"  - K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm tra: {TEST_SIZE:.1%}\n")
        f.write(f"  - S·ªë l·ªõp BI-RADS: {num_classes}\n")
        f.write(f"  - S·ªë l∆∞·ª£ng c√¢y t·ªëi ∆∞u (best iteration + 1): {optimal_num_trees}\n")
        f.write("-"*53 + "\n\n")
        if metrics_calculated:
            f.write(f"ƒê·ªô ch√≠nh x√°c (Accuracy): {accuracy:.4f}\n\n")
            f.write("B√°o c√°o ph√¢n lo·∫°i chi ti·∫øt (Classification Report):\n")
            f.write(report_str)
            f.write("\n\nMa tr·∫≠n nh·∫ßm l·∫´n (Confusion Matrix):\n")
            f.write(cm_df.to_string())
            f.write("\n\n")
            if plot_evaluation_charts:
                f.write("C√°c bi·ªÉu ƒë·ªì ƒë√°nh gi√°:\n")
                f.write(f"  - Bi·ªÉu ƒë·ªì hi·ªáu su·∫•t hu·∫•n luy·ªán: {output_training_plot_path}\n")
                f.write(f"  - Ma tr·∫≠n nh·∫ßm l·∫´n: {output_cm_plot_path}\n")
                f.write(f"  - ƒê∆∞·ªùng cong ROC: {output_roc_plot_path}\n")
                f.write(f"  - ƒê∆∞·ªùng cong Precision-Recall: {output_pr_plot_path}\n")
            else:
                f.write("C√°c bi·ªÉu ƒë·ªì (Loss, CM) ƒë√£ ƒë∆∞·ª£c l∆∞u. ROC/PR kh√¥ng v·∫Ω do l·ªói ho·∫∑c thi·∫øu x√°c su·∫•t.\n")
            f.write("\nGi·∫£i th√≠ch LIME ({len(lime_plot_paths)} bi·ªÉu ƒë·ªì):\n")
            if lime_plot_paths:
                for path in lime_plot_paths:
                    f.write(f"  - Bi·ªÉu ƒë·ªì LIME: {path}\n")
            else:
                f.write("  - Kh√¥ng c√≥ bi·ªÉu ƒë·ªì LIME do l·ªói.\n")
            f.write("\nGi·∫£i th√≠ch SHAP:\n")
            if shap_plot_path:
                f.write(f"  - Bi·ªÉu ƒë·ªì SHAP t√≥m t·∫Øt to√†n b·ªô m√¥ h√¨nh: {shap_plot_path}\n")
            else:
                f.write("  - Kh√¥ng c√≥ bi·ªÉu ƒë·ªì SHAP t√≥m t·∫Øt do l·ªói.\n")
            f.write(f"  - Bi·ªÉu ƒë·ªì SHAP waterfall ({len(shap_waterfall_paths)} bi·ªÉu ƒë·ªì):\n")
            if shap_waterfall_paths:
                for path in shap_waterfall_paths:
                    f.write(f"    - Bi·ªÉu ƒë·ªì SHAP waterfall: {path}\n")
            else:
                f.write("    - Kh√¥ng c√≥ bi·ªÉu ƒë·ªì SHAP waterfall do l·ªói.\n")
        else:
            f.write("!!! L·ªñI TRONG QU√Å TR√åNH D·ª∞ ƒêO√ÅN HO·∫∂C T√çNH TO√ÅN METRICS !!!\n")
            f.write("   Vui l√≤ng ki·ªÉm tra l·∫°i log l·ªói tr√™n m√†n h√¨nh.\n")
        f.write("="*53 + "\n")
    print(f"     üíæ L∆∞u b√°o c√°o th√†nh c√¥ng v√†o: {output_report_path}")
except Exception as e:
    print(f"     ‚ö†Ô∏è L·ªói khi l∆∞u file b√°o c√°o: {e}")
end_step_time = time.time()
print(f"‚úÖ L∆∞u k·∫øt qu·∫£ ho√†n t·∫•t (Th·ªùi gian: {end_step_time - start_step_time:.2f}s).")
print("\n-------------------------------------------------------------")
print("--- Quy tr√¨nh K·∫øt h·ª£p v√† Ph√¢n lo·∫°i BI-RADS Ho√†n t·∫•t ---")
print("-------------------------------------------------------------")